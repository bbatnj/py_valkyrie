{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13e56302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/veracrypt1/python/valkyrie/apps/common/run_notebook_import.py:11: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run /home/bb/src/python/valkyrie/apps/common/run_notebook_import.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a91defd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/veracrypt1/python/valkyrie/lib/valkyrie/nibelungen/data.py:192: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @jit\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from itertools import product as cartesian_product\n",
    "\n",
    "from overrides import overrides\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from valkyrie.tools import *\n",
    "from valkyrie.ml import modules\n",
    "from valkyrie.ml.utils import tensor, HyperParameters\n",
    "from valkyrie.ml import utils as ml_utils\n",
    "from valkyrie.nibelungen.data import DataMgr, Df2T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83dad404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after dropping na 2678245 -> 2645478\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 2645478 and the array at index 1 has size 2678245",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m channels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuy_qty_sum\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msell_qty_sum\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     12\u001b[0m n_channels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(channels)\n\u001b[0;32m---> 13\u001b[0m df2t2 \u001b[38;5;241m=\u001b[39m Df2T2(df_res, M \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m, xcols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuy_qty_sum\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msell_qty_sum\u001b[39m\u001b[38;5;124m'\u001b[39m], ycol \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmid_last_ret_120_n\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     14\u001b[0m               wcol \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwgt_mid_last_ret_120_n\u001b[39m\u001b[38;5;124m'\u001b[39m, mul \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/media/veracrypt1/python/valkyrie/lib/valkyrie/nibelungen/data.py:183\u001b[0m, in \u001b[0;36mDf2T2.__init__\u001b[0;34m(self, df, M, xcols, ycol, wcol, mul, dtype, device, yscaler)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# X.shape is now (n_features, M, n_samples)\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myw \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39mc_[yscaler \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf[ycol]\u001b[38;5;241m.\u001b[39mvalues, weights], dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/lib/index_tricks.py:419\u001b[0m, in \u001b[0;36mAxisConcatenator.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m scalars:\n\u001b[1;32m    417\u001b[0m         objs[k] \u001b[38;5;241m=\u001b[39m objs[k]\u001b[38;5;241m.\u001b[39mastype(final_dtype)\n\u001b[0;32m--> 419\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconcatenate(\u001b[38;5;28mtuple\u001b[39m(objs), axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m matrix:\n\u001b[1;32m    422\u001b[0m     oldndim \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mndim\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 2645478 and the array at index 1 has size 2678245"
     ]
    }
   ],
   "source": [
    "sdate, edate = '2023-01-01', '2023-01-31'\n",
    "instr = 'BDM.BTC.USDT.FP'\n",
    "freq = '1s'\n",
    "ret_n_s = [30]\n",
    "\n",
    "#########################################################\n",
    "#DataMgr\n",
    "#########################################################\n",
    "data_mgr = DataMgr(sdate, edate, freq, '/home/bb/data/BDM', instrs = ['BDM.BTC.USDT.FP'], ret_n_s = [120])\n",
    "df_res = data_mgr.get(instr) #'bpx_last','apx_last','bq_last','aq_last',\n",
    "channels = ['buy_qty_sum', 'sell_qty_sum']\n",
    "n_channels = len(channels)\n",
    "df2t2 = Df2T2(df_res, M = 16, xcols = ['buy_qty_sum', 'sell_qty_sum'], ycol = 'mid_last_ret_120_n',\n",
    "              wcol = 'wgt_mid_last_ret_120_n', mul = 2, dtype = torch.float32, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba3de26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_df_mem_usage(df_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c3b693",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df2t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07716a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from valkyrie.ml.data import DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ee666a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DataModule.from_dataset(df2t2, None, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc0ffa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LrNet(modules.Regressor): \n",
    "    def __init__(self, lr):\n",
    "        print(\"Lr Net\")\n",
    "        super().__init__('l1')\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          nn.LazyLinear(1, bias=False)\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c63f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_net = LrNet(1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42f5eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LrNet(lr = 1e-2)\n",
    "X = df2t2[0][0]\n",
    "ml_utils.layer_summary(net, X.shape)\n",
    "ml_utils.parameter_summary(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a4134f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = modules.Trainer(max_epochs=2, num_gpus=1)\n",
    "with Timer('training'):\n",
    "    trainer.fit(net, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a501e897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_init_loss(net, dm):\n",
    "    init_loss, loss = 0.0, 0.0\n",
    "    dl = dm.get_dataloader(True)\n",
    "    for i, batch in enumerate(dl):\n",
    "        X, YW = batch[0], batch[-1]                \n",
    "        ZEROS = torch.zeros(YW.shape[0])\n",
    "        with torch.no_grad():            \n",
    "            init_loss += net.loss(ZEROS, YW)    \n",
    "            Y_hat = net(X.to(ml_utils.gpu()))\n",
    "            loss += net.loss(Y_hat, YW.to(ml_utils.gpu()))\n",
    "    return init_loss, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edb7e9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_init_loss(net, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e63c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_utils.parameter_summary(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62f8e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    print(f\"Parameter name: {name}, param: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44616b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "    # c1--c4 are the number of output channels for each branch\n",
    "    def __init__(self, c1, c2, c3, c4, **kwargs):\n",
    "        super(Inception, self).__init__(**kwargs)\n",
    "        # Branch 1\n",
    "        self.b1_1 = nn.LazyConv2d(c1, kernel_size=1)\n",
    "        # Branch 2\n",
    "        self.b2_1 = nn.LazyConv2d(c2[0], kernel_size=1)\n",
    "        self.b2_2 = nn.LazyConv2d(c2[1], kernel_size=3, padding=1)\n",
    "        # Branch 3\n",
    "        self.b3_1 = nn.LazyConv2d(c3[0], kernel_size=1)\n",
    "        self.b3_2 = nn.LazyConv2d(c3[1], kernel_size=5, padding=2)\n",
    "        # Branch 4\n",
    "        self.b4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.b4_2 = nn.LazyConv2d(c4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b1 = F.relu(self.b1_1(x))\n",
    "        b2 = F.relu(self.b2_2(F.relu(self.b2_1(x))))\n",
    "        b3 = F.relu(self.b3_2(F.relu(self.b3_1(x))))\n",
    "        b4 = F.relu(self.b4_2(self.b4_1(x)))\n",
    "        return torch.cat((b1, b2, b3, b4), dim=1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):        \n",
    "        b1 = F.relu(self.b1_1(x))\n",
    "        b2 = F.relu(self.b2_2(F.relu(self.b2_1(x))))\n",
    "        b3 = F.relu(self.b3_2(F.relu(self.b3_1(x))))\n",
    "        b4 = F.relu(self.b4_2(self.b4_1(x)))\n",
    "        return torch.cat((b1, b2, b3, b4), dim=1)\n",
    "\n",
    "class GoogleNet(modules.Regressor):\n",
    "    def b1(self):\n",
    "        return nn.Sequential(            \n",
    "            nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "@d2l.add_to_class(GoogleNet)\n",
    "def b2(self):\n",
    "    return nn.Sequential(        \n",
    "        nn.LazyConv2d(64, kernel_size=1), nn.ReLU(),\n",
    "        nn.LazyConv2d(192, kernel_size=3, padding=1), nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "@d2l.add_to_class(GoogleNet)\n",
    "def b3(self):\n",
    "    return nn.Sequential(        \n",
    "        Inception(64, (96, 128), (16, 32), 32),\n",
    "        Inception(128, (128, 192), (32, 96), 64),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "@d2l.add_to_class(GoogleNet)\n",
    "def b4(self):\n",
    "    return nn.Sequential(\n",
    "                         Inception(192, (96, 208), (16, 48), 64),\n",
    "                         Inception(160, (112, 224), (24, 64), 64),\n",
    "                         Inception(128, (128, 256), (24, 64), 64),\n",
    "                         Inception(112, (144, 288), (32, 64), 64),\n",
    "                         Inception(256, (160, 320), (32, 128), 128),\n",
    "                         nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "@d2l.add_to_class(GoogleNet)\n",
    "def b4(self):\n",
    "    return nn.Sequential(\n",
    "                         Inception(192, (96, 208), (16, 48), 64),\n",
    "                         Inception(160, (112, 224), (24, 64), 64),\n",
    "                         Inception(128, (128, 256), (24, 64), 64),\n",
    "                         Inception(112, (144, 288), (32, 64), 64),\n",
    "                         Inception(256, (160, 320), (32, 128), 128),\n",
    "                         nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "@d2l.add_to_class(GoogleNet)\n",
    "def b5(self):\n",
    "    return nn.Sequential(\n",
    "                         Inception(256, (160, 320), (32, 128), 128),\n",
    "                         Inception(384, (192, 384), (48, 128), 128),\n",
    "                         nn.AdaptiveAvgPool2d((1,1)), nn.Flatten())\n",
    "\n",
    "@d2l.add_to_class(GoogleNet)\n",
    "def b0(self):\n",
    "    return nn.BatchNorm2d(2);\n",
    "\n",
    "\n",
    "\n",
    "@d2l.add_to_class(GoogleNet)\n",
    "def __init__(self, lr=0.1, num_classes=10):\n",
    "    #super(GoogleNet, self).__init__()\n",
    "    \n",
    "    super(GoogleNet, self).__init__('l1')        \n",
    "    self.save_hyperparameters()\n",
    "    self.net = nn.Sequential(self.b0(), self.b1(), self.b2(), self.b3(), self.b4(),\n",
    "                             self.b5(), nn.LazyLinear(1))\n",
    "    self.net.apply(d2l.init_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262a699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):  #@save\n",
    "    \"\"\"The Residual block of ResNet models.\"\"\"\n",
    "    def __init__(self, num_channels, use_1x1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1,\n",
    "                                   stride=strides)\n",
    "        self.conv2 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1,\n",
    "                                       stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.LazyBatchNorm2d()\n",
    "        self.bn2 = nn.LazyBatchNorm2d()\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        Y += X\n",
    "        return F.relu(Y)\n",
    "    \n",
    "class ResNet(modules.Regressor):\n",
    "    def b1(self):\n",
    "        return nn.Sequential(\n",
    "            nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.LazyBatchNorm2d(), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    \n",
    "@d2l.add_to_class(ResNet)\n",
    "def block(self, num_residuals, num_channels, first_block=False):\n",
    "    blk = []\n",
    "    for i in range(num_residuals):\n",
    "        if i == 0 and not first_block:\n",
    "            blk.append(Residual(num_channels, use_1x1conv=True, strides=2))\n",
    "        else:\n",
    "            blk.append(Residual(num_channels))\n",
    "    return nn.Sequential(*blk)\n",
    "\n",
    "@d2l.add_to_class(ResNet)\n",
    "def __init__(self, arch, lr=0.1, num_classes=10):\n",
    "    super(ResNet, self).__init__('l1')\n",
    "    self.save_hyperparameters()\n",
    "    self.net = nn.Sequential(self.b1())\n",
    "    for i, b in enumerate(arch):\n",
    "        self.net.add_module(f'b{i+2}', self.block(*b, first_block=(i==0)))\n",
    "    self.net.add_module('last', nn.Sequential(\n",
    "        nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
    "        nn.LazyLinear(num_classes)))\n",
    "    self.net.apply(d2l.init_cnn)\n",
    "    \n",
    "class ResNet18(ResNet):\n",
    "    def __init__(self, lr=0.1):\n",
    "        super().__init__(((2, 64), (2, 128), (2, 256), (2, 512)),\n",
    "                       lr, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342e6e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_net = ResNet18(lr = 1e-4)\n",
    "X = df2t2[0][0]\n",
    "ml_utils.layer_summary(res_net, X.shape)\n",
    "ml_utils.parameter_summary(res_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05560235",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_net = GoogleNet(lr = 0.1)\n",
    "X = df2t2[0][0]\n",
    "ml_utils.layer_summary(google_net, X.shape)\n",
    "ml_utils.parameter_summary(google_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1491862",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer = modules.Trainer(max_epochs=100, num_gpus=1)\n",
    "with Timer('training'):\n",
    "    trainer.fit(google_net, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff038272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_init_loss(net, dm):\n",
    "    init_loss, loss = 0.0, 0.0\n",
    "    dl = dm.get_dataloader(True)\n",
    "    for i, batch in enumerate(dl):\n",
    "        X, YW = batch[0], batch[-1]\n",
    "        ZEROS = torch.zeros(YW.shape[0])\n",
    "        with torch.no_grad():\n",
    "            init_loss += net.loss(ZEROS, YW)\n",
    "            Y_hat = net(X.to(ml_utils.gpu()))\n",
    "            loss += net.loss(Y_hat, YW.to(ml_utils.gpu()))\n",
    "            #Y_hat = lr.predict(X.view(-1, 6)) #net(X.to(ml_utils.gpu())).to(ml_utils.cpu())\n",
    "            #Y_hat = torch.tensor(Y_hat)\n",
    "            #Y_hat = YW[:,0]\n",
    "            #loss += net.loss(Y_hat, YW)                        \n",
    "    return init_loss, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28409f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_init_loss(google_net, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12cd4f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = modules.Trainer(max_epochs=100, num_gpus=1)\n",
    "with Timer('training'):\n",
    "    trainer.fit(google_net, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c7e2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = dm.get_dataloader(train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a174a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b2eb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_batch = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3930293c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1202c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_batch[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6fbc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e6a28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from valkyrie.ml.utils import to_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c522c11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = google_net\n",
    "dl = dm.get_dataloader(train=True)\n",
    "\n",
    "# Define loss function and optimizer (assuming Mean Squared Error and SGD optimizer)\n",
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop (Example: training for 100 epochs)\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'epoch = {epoch}')\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    iterator = iter(dl)\n",
    "    epoch_loss = 0.0\n",
    "    for batch_idx, random_batch in enumerate(dl):           \n",
    "        x = random_batch[0]\n",
    "        y = random_batch[1][:,0]\n",
    "        x = to_gpu(x)\n",
    "        y = to_gpu(y)\n",
    "        outputs = model(x)\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs.squeeze(), y)  # Assuming single-value prediction\n",
    "        epoch_loss += loss        \n",
    "        # Zero gradients, backward pass, and optimize\n",
    "        #optimizer.zero_grad()\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "        \n",
    "    print(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7943a861",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
